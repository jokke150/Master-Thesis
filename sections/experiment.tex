\chapter{Experimental setup}
\label{sec:setup}

In this chapter, the experimens that are performed are introduced. First, an overview of the different evaluated scenarios is given. Subsequently, some important experiment design decisions are explained and justified. Furthermore, the process of tuning the hyperparameters is discussed. Lastly, the performance metric for the evaluation is presented.

All experiments have been performed single threaded, but in parallel, on a Google Cloud C2 Compute-optimized\footnote{See \href{https://cloud.google.com/compute/docs/machine-types}{Google Cloud machine families}} virtual machine with 60 cores and 240 GB RAM, running Ubuntu 20.04 LTS.


% TODO: Add recap of general highway lane keeping scenario?

\section{Evaluated scenarios}

Three different agent configurations are evaluated, with differing action selection procedures. Moreover, there are three driver models with increasing complexity. In the experiments, each agent is tested with each of the driver models. The aim is to find out how well the different agents can handle the increasing complexity of the driver models.

The three agents that are considered are not fundamentally different. They all use the POMCP algorithm. The difference lies in the way they select actions. The first agent utilizes the full action space. During rollouts, it chooses actions uniformly random. The second agent only considers a subset of the action space containing only minor steering actions (see Table \ref{tab:actions}). For both the first and the second agent, the actions are given an initial value of zero. The third agent uses preferred actions (see Section \ref{sec:preferred_actions}). It considers the full action space but assigns different probabilities to the actions for their selection during rollouts, preferring minor steering actions over strong ones. Furthermore, it assigns initial values to the action nodes (see Table \ref{tab:pref_actions}).

The driver model configurations are introduced in Section \ref{sec:driver_model}. The basic driver model acts optimally when the driver is attentive and, when distracted, the last action which was performed while the driver was still attentive is repeated. The second model adds complexity by introducing a random amount of steering overcorrection when a formerly distracted driver regains her attentiveness. The intuition is that  a driver who suddenly realizes that she has deviated from the lane center is startled and thus steers too strongly to correct the car's position. The third driver model is based on the second one and adds a random amount of action noise on top. This noise is meant to make the driver's behavior more realistic and less predictable.

\section{Experiment design decisions}
% A number of decisions that were made about the experiments are especially noteworthy. 

Nine scenarios were defined in the last section (three agents $\times$ three driver models). For each of these scenarios, 13 experiments are executed with varying number of forward searches during planning with the POMCP algorithm (see Section \ref{sec:pomcp}). The rationale behind this is to determine after how many searches the agents' performance converges. The number of searches has a substantial impact on the planning time the agent needs before deciding on its next action. The fewer searches are needed for a good policy, the better, as this translates to a lower planning time. Table \ref{tab:num_searches} shows the 13 values that are used in the experiments. The computational complexity of the planning process grows exponentially with the number of searches. Therefore, it was decided to increase the relative distance between the values as the values increase. This prevent a precise determination of the convergence point. To further narrow down the convergence point, additional experiments would be required. In practice, a maximum planning time is often defined after which planning ends, no matter how many searches have been performed \parencite{pomcp}. This is not the case for the experiments in this thesis.

\input{sections/tables/num_searches.tex}

Every experiment consists of 100 repetitions (runs) of episodes consisting of 1000 actions each, if no terminal state is reached earlier. The episodes are independent of each other - no data is persisted. Each episode starts with the same car state (centered in the lane, relative yaw angle of zero) and the same driver model state (attentive, same duration until distracted). The accelleration and braking are controlled by an external controller, keeping the speed constant at 80 km/h during the entire episode. An episode can end prematurely if a terminal state is reached. A state is terminal if the car is off track, which is considered to be the case if the center of the car lies within more than 20 cm beside the left or right lane markings.

Every run has a different seed for the random number generator used for all randomness in the driver model. Every run comes with different driver behavior. However, all experiments share the same seeds for the runs. For example, this means that during run one of every experiment, the driver model state changes are at the same times and lead to the same durations for attentive and distracted periods. The driver actions can differ though since they are dependent on the car's state, which is also influenced by the agent's actions. The reason for this setup is twofold: First, varying driver behavior for different runs verifies the robustness of the agents to varying situations. Second, sharing the seeds between different experiments ensures the comparability between experiments. The driver model used for the generative model of the agent and the driver model representing the actual driver do \emph{not} share the same seed.

1000 actions represent 100 seconds of simulated driving time. The simulation is not executed in real time. For the time of the planning of the agent, the simulation \emph{waits} for the agent; the simulation does not proceed further until the agent has decided on an action. By default, TORCS expects drivers to input new steering actions every 0.02 seconds. However, for the experiments, the action frequency for the agent and the driver model was decreased to 0.1 seconds. The simulation progresses during this time (see Section \ref{sec:state}). This means that every combined action is repreated for 0.1 seconds. There are two reasons for this: First, this enables the evaluation of the agent's performance with a longer driving time. Especially for large number of searches, the planning time for every action is rather large. Getting data for 100 seconds of driving time with an action frequency of 0.02 would require five times the current computation time. This is not feasible with the limited recourses for this thesis. Second, lowering the action frequency makes the driving task more difficult as it is easier to overshoot when the same action is repeated often. The agent has to plan more carefully.  Moreover, the agent has to look further ahead as there are more situations in which the action that leads to the highest immediate reward is not the best to choose to maximize long-term return.

The actions of the driver are discrete (see Section \ref{sec:driver_act_discr}). Obviously, this is not very realistic. The discretization of the driver's actions decreases the complexity of the problem sufficiently. It is conceivable that POMCP could be applied even if the driver's actions were continuous. Continuous driver actions, rather than continuous agent actions, do not have the effect of limiting the search tree depth because of a width explosion (see Section \ref{sec:discretization}). However, they make it a lot harder to predict the driver's actions precisely. None of the agens is able to plan with continuous driver actions. 

\section{Parameters and hyperparameter optimization}

% TODO: Add parameter and hyperparameter table

% TODO: Refine
There are a number of hyperparameters. Most importantly, there is the planning time. This is the time the agent is allowed to search in and expand its search tree in order to find the most likely current state and best course of action. More planning time can result in a wider and/or deeper tree. The search horizon is limited by a discount threshold. If this threshold is reached, the search is stopped and no more actions will be performed for the current trajectory and, if there is enough planning time left, a new state is sampled from the current belief and a new planning trajectory is expanded. Moreover, there is an exploration constant. This value, determined before the start of the experiment, assigns actions that have not been tried before more expected reward and thus favors exploration.
\subsubsection{Number of searches}
\subsubsection{Exploration constant}
\subsubsection{Discount horizon}

\section{Performance metrics}
Due to the randomness involved in the driver model, each experiment run will lead to a different scenario. Therefore, to get credible results, the experiment has to be repeated many times. The average discounted return over all experiment runs serves as performance metric. This result is compared with the average reward of an agent that always performs the optimal reaction to the action of the driver model. The closer the POMCP agent's reward is to the optimal agent's reward, the better was the planning.

