\chapter{Experimental setup}
\label{ch:setup}

% Accelleration, braking, and gear changes are performed by a simple controller intended to keep the speed constant at at all times. 

% TODO: Clear message: What do we want to find out?

% TODO: Move to experiment stup

% Since the problem is a lane-keeping task, any state representing a lane departure is considered a \emph{terminal state}, resulting in the end of an experiment. The car is considered to have departed from its lane if the center of car lies more than 20cm beyond side lane markings.

% The state is updated by the simulation engine every 0.002 seconds of simulated time (this is the driving time that is simulated, not real time). By default, robots provide new control actions to the car every 0.02 seconds. However, for the experiments in this thesis, a rate of 0.1 seconds is chosen. This makes the ride less smooth but reduces the frequency in which the agent has to plan ahead, reducing the amount of planning time. The steering action is repeated until a new one is provided.

% The driver model is initially set to an attentive state. 

% Every 0.1 seconds, an action is chosen depending on the driver's current state. Afterwards, the remaining duration is decremented by 0.1.


In this chapter, the experimens that are performed are introduced. First, an overview of the different evaluated scenarios is given in section \ref{sec:scenarios}. Subsequently, in section \ref{sec:design_decisions} some important experiment design decisions are explained and justified. Furthermore, the process of tuning the hyperparameters is discussed in section \ref{sec:exp_hyperparams}. Lastly, the performance metrics for the evaluation are presented in section \ref{sec:perf_metrics}.

\emph{All experiments have been performed single threaded, but in parallel, on a Google Cloud C2 Compute-optimized\footnote{See \href{https://cloud.google.com/compute/docs/machine-types}{Google Cloud machine families}} virtual machine with 60 cores and 240 GB RAM, running Ubuntu 20.04 LTS.}


% TODO: Add recap of general highway lane keeping scenario?

\section{Evaluated scenarios}
\label{sec:scenarios}

Three different agent configurations are evaluated, with differing action selection procedures. Moreover, there are three driver models with increasing complexity. In the experiments, each agent is tested with each of the driver models. The aim is to find out how well the different agents can handle the increasing complexity of the driver models.

The three agents that are considered are not fundamentally different. They all use the POMCP algorithm. The difference lies in the way they select actions. The first agent utilizes the full action space. During rollouts, it chooses actions uniformly random. The second agent only considers a subset of the action space containing only minor steering actions (see table \ref{tab:actions}). For both the first and the second agent, the actions are given an initial value of zero. The third agent uses preferred actions (see section \ref{sec:preferred_actions}). It considers the full action space but assigns different probabilities to the actions for their selection during rollouts, preferring minor steering actions over strong ones. Furthermore, it assigns initial values to the action nodes (see table \ref{tab:pref_actions}).

The driver model configurations are introduced in section \ref{sec:driver_model}. The basic driver model acts optimally when the driver is attentive and, when distracted, the last action which was performed while the driver was still attentive is repeated. The second model adds complexity by introducing a random amount of steering overcorrection when a formerly distracted driver regains her attentiveness. The intuition is that  a driver who suddenly realizes that she has deviated from the lane center is startled and thus steers too strongly to correct the car's position. The third driver model is based on the second one and adds a random amount of action noise on top. This noise is meant to make the driver's behavior more realistic and less predictable.

In total, this maks nine scenarios (three agents $\times$ three driver models). For each of these scenarios, 13 experiments are executed with varying number of forward searches during planning with the POMCP algorithm (see section \ref{sec:pomcp}). The rationale behind this is to determine after how many searches the agents' performance converges. The number of searches has a substantial impact on the planning time the agent needs before deciding on its next action. The fewer searches are needed for a good policy, the better, as this translates to a lower planning time.

\section{Experiment design decisions}
\label{sec:design_decisions}
A number of noteworthy experiment design decisions were made that are important to consider. These are highlighted and motivated in the following subsections.

\subsection{Episode definition and repetitions}

%TODO: Mention that we choose a challenging task. Usually distracted drivers don't get off the road within seconds ^^

% Most IVIS tasks increased the lateral deviations, but in the majority of cases the drivers were still able to stay in their lane.
% Driver distraction based lane-keeping assistance - C. Blaschke 

Every experiment consists of 100 repetitions (runs) of episodes consisting of 1000 actions each, if no terminal state is reached earlier. The episodes are independent of each other - no data is persisted. Each episode starts with the same car state (centered in the lane, relative yaw angle of zero) and the same driver model state (attentive, same duration until distracted). The accelleration and braking are controlled by an external controller, keeping the speed constant at 80 km/h during the entire episode. An episode can end prematurely if a terminal state is reached. A state is terminal if the car is off track, which is considered to be the case if the center of the car lies within more than 20 cm beside the left or right lane markings.

1000 actions represent 100 seconds of simulated driving time. The simulation is not executed in real time. For the time of the planning of the agent, the simulation \emph{waits} for the agent; the simulation does not proceed further until the agent has decided on an action. 

\subsection{Choice of evaluated number of searches}

\input{sections/tables/num_searches.tex}

table \ref{tab:num_searches} shows the 13 values that are used in the experiments for the number of searches during planning. The computational complexity of the planning process grows exponentially with the number of searches. Therefore, it was decided to increase the relative distance between the values as the values increase. This prevent a precise determination of the convergence point. To further narrow down the convergence point, additional experiments would be required. In practice, a maximum planning time is often defined after which planning ends, no matter how many searches have been performed \parencite{pomcp}. This is not the case for the experiments in this thesis.

\subsection{Initial belief}

The initial belief of the agent consists of 1000 particles that each consist of a randomly sampled attentive driver model state and a car state that is slighly modified but close to the true car state. This may seem like cheating at firs but providing completely random car states to the agent would be like asking it to search for a needle in a haystack. The state space is just too large. The main objective for the agent is to cope with the unpredictability of and partial information about the driver's attentiveness.

The agent knows that the driver is initially attentive but not for how long. The duration until the next change of attentiveness occurs is randomly sampled but adheres to the usual dynamics of the driver model; the duration may not be larger than the maximum or smaller than the minimum defined in section \ref{sec:driver_state}. The car states for the initial belief are created by slighly modifying the real initial car state. For every of the 1000 potential initial states, the lane centeredness and the angle of the real car state are increased or decreased by a randomly chosen amount of up to five percent in either direction. Thereby, the agent does not have perfect knowledge of the car's current lateral position but knows how far it is on the track and how fast it is going.

\subsection{Decreasing the steering frequency}

By default, TORCS expects drivers to input new steering actions every 0.02 seconds. However, for the experiments, the action frequency for the agent and the driver model was decreased to 0.1 seconds. The simulation progresses during this time (see section \ref{sec:state}). This means that every combined action is repreated for 0.1 seconds. There are two reasons for this: First, this enables the evaluation of the agent's performance with a longer driving time. Especially for large number of searches, the planning time for every action is rather large. Getting data for 100 seconds of driving time with an action frequency of 0.02 would require five times the current computation time. This is not feasible with the limited recourses for this thesis. Second, lowering the action frequency makes the driving task more difficult as it is easier to overshoot when the same action is repeated often. The agent has to plan more carefully.  Moreover, the agent has to look further ahead as there are more situations in which the action that leads to the highest immediate reward is not the best to choose to maximize long-term return.

\subsection{Driver action discretization}

The actions of the driver are discrete (see section \ref{sec:driver_act_discr}). Obviously, this is not very realistic. The discretization of the driver's actions decreases the complexity of the problem sufficiently. It is conceivable that POMCP could be applied even if the driver's actions were continuous. Continuous driver actions, rather than continuous agent actions, do not have the effect of limiting the search tree depth because of a width explosion (see section \ref{sec:discretization}). However, they make it a lot harder to predict the driver's actions precisely. None of the agens is able to plan with continuous driver actions. 

\subsection{Controling randomness to ensure comparability}

Every run has a different seed for the random number generator used for all randomness in the driver model. Every run comes with different driver behavior. However, all experiments share the same seeds for the runs. For example, this means that during run one of every experiment, the driver model state changes are at the same times and lead to the same durations for attentive and distracted periods. The driver actions can differ though since they are dependent on the car's state, which is also influenced by the agent's actions. The reason for this setup is twofold: First, varying driver behavior for different runs verifies the robustness of the agents to varying situations. Second, sharing the seeds between different experiments ensures the comparability between experiments. The driver model used for the generative model of the agent and the driver model representing the actual driver do \emph{not} share the same seed.

\section{Hyperparameter optimization}
\label{sec:exp_hyperparams}

Besides the number of searches, there are two additional hyperparameters of POMCP that have a strong influence on the planning performance: The search horizon and the exploration constant. The exploration constant is used to enhance the value of rarely-tried actions in order to facilitate exploration and stop the agent from overfitting. The search horizon (or discount horizon) describes how many actions the agent looks into the future during planning, if no terminal state is reached earlier.

Six different exploration constants and three different search horizons are evaluated in a grid search for the best combination. Each combination is evaluated by the average cumulative return from an experiment with 20 runs, up to 1000 actions each, and 1000 searches per planning step. The second driver model is used for all experimens (steering overcorrection, no action noise; see section \ref{sec:driver_model_config}). The grid search is only performed for this selected scenario because of limited computing and time resources. However, the experiments should be sufficient to indicate the performance differences stemming from the hyperparameter combinations.

\section{Performance metrics}
\label{sec:perf_metrics}

% TODO: This is probably not great. Rework this section a little

The performance of the agents is measured in two ways: First, there is the cumulative reward an agent receives in an experiment for a particular scenario. Second, the terminal states an agent encounters during the experiment is considered. Terminal states entail that the lane keeping task was failed. It is to be expected that all agents will lead to similar cumulative rewards with an increasing amount of searches during planning. The key is the number of searches they require in order to achieve this reward. A lower number of searches means more efficient, faster planning. Furthermore, the optimality of the agents' policies when converged will be evaluated by comparing the agents' performances with two baselines. First, as lower bound, the driver model driving without assistance. Second, as upper bound, an agent with perfect knowldge of the state who always acts optimally.

% TODO: Incude Lane centeredness comparison?
