\chapter{Literature}
\label{sec:literature}

\iffalse

% FEEDBACK
% TODO: ACT-R Sources and better explanation
% TODO: Explain link between cogmod and rl agent


Model-based or model-free?

TODO: ADD HMM
Advantages of \gls{pomdp} over HMM:
Instead of single predicted state, a probability distribution over the possible states is considered
Allows the agent to actively take an action with the goal of reducing uncertainty about the human's state.

The scenario of a car with a human driver that is assisted by a \gls{rl} agent acting as \gls{adas} can be described as a \gls{pomdp}. From the perspective of the agent, the human is part of its environment. The agent can observe the car's sensory information and can perceive the human's actions. However, the agent is not aware if the human driver is attentive. Based on its observations, the agent must derive a probability distribution over the driver's level of attention (belief). Utilizing this, it tries to maximize its reward.

The \gls{pomdp} framework for \gls{hitl} control systems proposed by \cite{hitl_pomdp} serves as a foundation for this thesis. The driving task is extended from pure lane keeping, in a single lane without other road users, to multiple lanes with potential human-decided lane-switches, and other traffic participants with whom collisions have to be avoided. Moreover, continuous state and action spaces are considered.

\fi

% TODO reorder

This thesis focuses on efficient online \gls{pomdp} planning. The two most notable fast online \gls{pomdp} algorithms are DESPOT \parencite{despot} and POMCP \parencite{pomcp}. Both apply Monte Carlo tree search to evaluate the quality of candidate policies. At each time point, a simulator of the environment is used to form a search tree from multiple simulations in order to evaluate the resulting hypothetical histories by their mean return, leading to a real action of the agent in the environment and thus to a new real observation \parencite{pomcp}. DESPOT addresses and improves upon POMCP's problem of a poor worst-case performance bound \parencite{despot}.

Both POMCP and DESPOT can handle continuous state spaces but would have to be modified in order to support continuous action or observation spaces \parencite{online_pomdp_cont}. \citeauthor{online_pomdp_cont} provide two online algorithms for \gls{pomdp} with continuous state, action, and observation spaces: POMCPOW for simulating approximate state trajectories, and PFT-DPW for simulating approximate belief trajectories.

Offline and online approaches can be combined by using an approximate policy computed offline as a default policy \parencite{comb_online_offline}, or by considering a sequence of macro-actions to reduce the size of the search horizon \parencite{macro_actions}. Especially when there is only very little time for online planning, incorporating an offline approximation into an online approach can be useful \parencite{online_pomdp}.

\cite{hitl_pomdp} provide a framework for using a \gls{pomdp} to model a \gls{hitl} control system. Their framework serves as foundation for this thesis and is further described in \cref{sec:problem}. The framework is used in a case study where an agent assists a potentially drowsy human driver in keeping the car centered in its lane. Whether or not the driver is drowsy remains unknown to the agent. The agent's estimation of the driver's drowsiness is based on the humans actions such as turning the steering wheel and opening or closing its eyes. Intervention by the agent is possible both by alerting the driver with a warning, and by actively steering the car. Any intervention is penalized with the aim to interfere with the driver as little as possible but as much as required in order to keep the car centered when the driver is drowsy. They approximately solve their \gls{pomdp} problem with an offline randomized point-based value iteration approach. The policy is computed by iteratively sampling a finite set of random points from the agent's belief space. The agent thus interacts randomly with the environment in order to find an approximation of the optimal policy. The employed model of the human's internal state is rather simplistic and based on handcrafted transition probabilities. The state and action spaces are discrete.

\cite{att_intersec} evaluate how active probing can be utilized by autonomous vehicles in driving scenarios to reduce their uncertainty about a hidden psychological state of human drivers on the road. Three different scenarios are modeled: First, the agent wants to cross an intersections with other cars driving on the crossed road. The autonomous car can cautiously approach in order to probe the other cars for attentiveness; if they react by reducing their speed, they are likely attentive. Second, the agent drives on a highway with human drivers approaching from behind. The goal is to avoid rear-end collisions, which are especially likely in the case of inattentive human drivers. Active probing can be performed by the agent through braking. If an approaching driver does not slow down, the driver is most likely not paying attention. Third, the agent actively probes for an aggressive or timid driving style of other drivers by nudging into their lane. A human driver is expected to either slow down to allow the agent to switch lanes (timid driving style), or speed up to discourage the agent to switch lanes (agressive). This approach of actively provoking human responses rather than just passively observing leads to a significant improvement in classifying the human drivers' hidden attentiveness. It can potentially also be utilized in a shared control setting. The agent could utilize minor interventions to reduce uncertainty about the human's internal state.

Furthermore, the work of \cite{att_intersec} is relevant with regard to how they represent their problem as a \gls{pomdp} with a continuous state and action space and plan online using \gls{mpc}. At every time step, the agent uses an embedded human model to make predictions over a finite horizon about the actions a human would take in response to its own actions. The agent knows how a human would act in different psychological states, the state itself, however, is hidden. It is assumed that the human always tries to maximize its reward. The agent chooses a policy that maximizes its reward while accounting for the human's (potential) actions depending on the hidden psychological state the agent believes the human to be in. The real human actions that are observed after the agent executes an action are used to update the agent's belief about the human's internal state. The human model is learned a priori using \gls{irl} using demonstrations of human behavior for which the human's internal state is known.


% Shared control

\cite{shared_control} provide an overview of papers regarding decision making and human driver modeling for driver-vehicle shared control scenarios. Insight about recent developments, different architectures, and remaining challenges is provided. Of particular relevance are the different modes for the communication of authority between human and agent and the cognitive modeling approaches that are discussed.

% TODO: give examples of architectures




