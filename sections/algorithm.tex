\chapter*{Algorithm}
\label{sec:algorithm}

% TODO: Look at PSRs

% The bane of generative model approaches is that they are often strongly dependent on a good model of the system’s dynamics. Most uses of partially observable Markov decision processes (POMDPs), for example, assume a perfect dynamics model and attempt only to estimate state.

% behavior distributions of human drivers are unknown

\begin{itemize}
    \item Planning: Computational process that takes a model as input and produces or improves a policy for interacting with the modeled environment.
    \item Learning: Whereas planning methods use simulated experience generated by a model, learning methods use real experience generated by the environment.
\end{itemize}

\section{Model-based planning}

\subsection{Offline}

Only suitable for small to medium POMDP problems but very difficult to scale up to large problems as the number of future sceanarios that has to be considered grows exponentially with the size of the POMDP \parencite{despot}. Therefore, offline planning is not considered in this thesis. 

\subsection{Online}

\textbf{With black-box simulator:}
\begin{itemize}
    \item POMCP (Monte-Carlo tree search from the current belief using samples from a black-box simulator.)
    \item DESPOT (Avoids POMCP’s extremely poor worst-case behavior by evaluating policies on a small number of sampled scenarios.)
    \item DESPOT-IS (Importance sampling improves DESPOT’s performance when there are critical, but rare events, which are difficult to sample.)
    \item HyP-DESPOT (Parallelization speeds up online planning by up to several hundred times, compared with the original DESPOT algorithm.)
\end{itemize}

\subsection{Notes}

\begin{itemize}
    \item The black-box simulator might not be time-efficient enough to plan ahead sufficiently.
    \item Even if we would achieve an optimal policy for our simulation environment, the performance is bound by the accuracy of the ACT-R model in the human experiment. An improvement of the model is not possible as it is stationary.
\end{itemize}

\section{Model-free learning}

\begin{itemize}
    \item QMDP-Net (A QMDP-Net is an RNN that imposes structure priors for sequential decision making under partial observability. It embeds a Bayesian filter and the QMDP algorithm in the network. The hidden state of the RNN encodes the belief for POMDP planning.)
    \item BA-POMCP (Bayes-Adaptive Partially Observable Markov Decision Processes (BA-POMDPs) extend POMDPs to allow the model to be learned during execution. BA-POMCP extends the Monte-Carlo tree search method POMCP to BA-POMDPs.)
    \item FBA-POMCP (Monte-Carlo tree search method for F-POMDPs where the state if factored into multiple components. Only applicable to some specific domains where this factorization is possible.)
    \item \emph{PSR-MCTS} (Planning from scratch with model uncertainty, where an offline \emph{PSR model} were firstly learned and then combined with online Monte-Carlo tree search.)
\end{itemize}

\subsection{Notes}

\begin{itemize}
    \item Success in model-free reinforcement learning for POMDPs is pretty much limited to small-scale problems at this point.
    \item BA-POMDP: While the approaches listed above show promising performance on some problems the performance is very dependent on prior domain knowledge \parencite{ba-pomdp-online-offline}.
\end{itemize}