\chapter{Results}
\label{ch:results}

This chapter presents the results from the experiments. First, in section \ref{sec:perf_bounds} the performance of the driver without assistance, and of an agent that acts optimally, are established as lower and upper performance bounds respectively. In section \ref{sec:grid_search}, the results of the hyperparameter optimization are presented. Then, the influence on the performance of the number of searches during planning is evaluated in section \ref{sec:convergence}, with the aim of identifying the point of convergence for each agent. The cumulative reward but also the terminal states reached during experiments are taken into account. The results are grouped according to the different versions of the driver model. Lastly, in section \ref{sec:centeredness} the progression of the mean lane centeredness is shown for an increasing number of searches.

% TODO: New scores for baseline as it cannot be perfect 999 if it was true that the agent sometimes has to take suboptimal actions in order to receive more reward in the long run. THis is quite important because otherwise a discount horizon of 1 would be fully sufficient

% TODO: Add: The agent can only recognize a state change of the driver after the driver has performed the first action in this state. This is because there are no observable clues about the duration of the driver's attentiveness and distraction.

% TODO: Add time after which terminal states occur. Generally, a little more data on terminal states would be good probably.

\section{Lower and upper performance bound}
\label{sec:perf_bounds}

%TODO: Mention that we choose a challenging task. Usually distracted drivers don't get off the road within seconds ^^

The benchmark for the performance of the agents is the performance of the driver without assistance system as lower bound, and the performance of an agent that always reacts optimally to the driver's actions as upper bound. For both baselines, 50 runs with up to 1000 actions each, if no terminal state is reached earlier, are performed for each of the three driver models. 

In the case of the independent driver, no run was completed successfully. At some point in any run, the driver becomes distracted and fails to adjust to a change of the course of the road, leading to lane departure. Table~\ref{tab:driver_performance} shows that the more complex the driver model is, the worse is its performance. The simple driver model, and the driver model with steering overcorrection lead to a few runs with a relatively high number of successful actions before a lane departure occurs. After becoming distracted, the driver only repeats it's last attentive action. In some cases this means that the distracted driver just continues to steer straight which is less likely to lead to lane departure on the highway track than consecutively steering left or right.

%  TODO: The baseline is TOO GOOD!
The agent reacting optimally to the driver's actions has full knowledge about the car state, as well as the driver's next action. It always chooses the action that leads to the best possible combined action. It finishes every run successfully and leads to average cumulative rewards of $999.3$ for all driver models.

\input{sections/tables/driver_performance}

\section{Hyperparameter optimization}
\label{sec:grid_search}

Hyperparameter optimization is performed for agents with all three action configurations and the driver model with steering overcorrection but without noise. Grid search is used to search for the combination of search horizon and exploration constant that leads to the highest average cumulative reward after 20 runs, with up to 1000 actions each, and 1000 searches per planning step.

\input{sections/plots/grid_plot}

For the agent with a full, unweighted action space, two combinations lead to a sufficiently higher average cumulative reward than the others: The combination of a search horizon of 5 actions with an exploration constant of 0.75 leads to a reward of 587.67, and the combination of planning 10 actions ahead with an exploration constant of 5 results in a reward of 581.40. The shallower the search horizon, the less planning time is needed at every planning step as fewer actions need to be simulated. Thus, at the same performance, a lower search horizon is preferable. The combination of a search horizon of 5 actions and an exploration constant of 0.75 is used for the further experiments.

In the case of the agent restricted to using a subset of the action space, the combination of a search horizon of 5 actions and an exploration constant of 25 yields the highest average cumulative reward of 646.83. Only the combination of a search horizon of 25 actions with an exploration constant of 1.5 comes relatively close with a reward of 616.47. As a lower search horizon is preferable, the setup for the further evaluation for this agent is a search horizon of 5 actions with an exploration constant of 25. 

The best combination of search horizon and exploration constant for the agent with preferred actions is 25 actions and 1.5 respectively. This combination yields an average cumulative reward of 942.05 which is sufficiently higher than the return of any other combination. Consequently, this is the combination used for this agent in subsequent experiments.

\section{Convergence behavior}
\label{sec:convergence}

% TODO: Rename to something better?


% TODO: Add planning time

The agent's challenge is threefold: First, it must accurately determine where the car is located on the track. Its observations are accurate, however, because of the discretization, multiple actual positions map to the same observation (See section~\ref{sec:observations}). Second, it needs to estimate whether the driver is attentive or not. Third, it must decide on an appropriate action choice, taking into account future consequences of the chosen action. In order to achieve this, at each planning step, the agent can search ahead by simulating experiences starting from a sampled belief state. Performing more searches means evaluating more possible scenarios. In turn, more evaluated scenarios enable the agent to form a better policy. However, after some amount of searches, the information gain from additional searches decreases and performance is expected to converge \parencite{pomcp}. The number of searches is directly related to the planning time. Planning time is a limiting factor for the application of a planner. Thus, knowing after how many searches the performance converges, and therefore being able to choose the minimal number of searches to perform for a good result, is critical. Below, the convergence behavior of the three evaluated agents is assessed for the scenarios of the simple driver model, the driver model with steering overcorrection after regaining attentiveness, and the driver with overcorrection and noise.

\subsection{Simple driver model}

For the experiment with the simple driver model, using the full action space, utilizing only the subset of minor steering actions, and employing preferred actions lead to similar convergence behavior. Already with just 200 searches during planning, the average cumulative reward is above 600 for all agents. However, as it can be seen in Table~\ref{tab:simple_terminal}, the number of runs that lead to a terminal state is high. In the runs resulting in a terminal state, the agents are able to assist the drivers well at first but suffer from particle deprivation after some time (see section \ref{sec:particle_deprivation}). Then, their belief deviates noticably from the true states of the environment and the driver. The agents are not able anymore to make accurate assumptions about the state of the environment and whether the driver is attentive or not. Thus, they are rendered unable to decide on the right actions to keep the car centered. 

\input{sections/plots/sims_norm_plot}

The agent using the full range of actions already causes only four terminal states with 300 searches, whereas the other two agents need more searches to perform well. The best result for all three agents is achieved with 1500 searches. No run results in a terminal state. The agent with the full action space receives an average cumulative reward of 957.83, the agent with a reduced action space yields 981.99, and the agent using preferred actions gains 973.88. 

For more searches, the average cumulative rewards are similar for the agents with a subset of actions and preferred actions. They seem to have converged. In contrast, the agent with a complete action space encounters four terminal states in the trial with 5000 searches and six in the experiment with 7500 seaches. These terminal states are reached early on during the run - executing less than 50 actions before. They are caused by an extreme action of the agent that leads to the opposite of optimal steering behavior. The actions completely overrule the driver's actions who is even concentrated in three of the ten occasions. The reason for choosing these extreme actions is a poor belief that strongly deviates from the true states of the environment and the driver. The agent with preferred actions is less likely to perform extreme actions, and the agent with a subset of minor actions cannot do so. 

The agent restricted to use only a subset of minor actions reaches terminal states in two states in the experiments with 2500, 7500, and 10000 searches. These are not caused by particle deprivation. In all cases, the car is in a road bend and the driver is distracted, steering into the wrong direction. The agent performs its best possible action by steering as much as possible in the opposite direction than the driver. However, because the agent's range of actions is severely limited, the combination of the agent's and driver's actions is not enough to keep the car in the lane. 

Using preferred actions results in only one terminal state for experiments with 1500 searches or more. The reason, like for the terminal states reached by the agent without weighted actions, is particle deprivation. There are multiple occasions where a distracted driver steers into the wrong direction in a road bend and the agent is able to correct the steering by effectively reversing the driver's actions.

% TODO: Add #actions while attentive

% The simple driver model steers optimally when it is in an attentive state. In a distracted state, it simply repeats the last action it has performed while it was still attentive until it becomes attentive again. Therefore, the optimal policy for the agent is to only interfere when the driver is distracted. 

\input{sections/tables/terminal_norm}

\subsection{Steering overcorrection}

A driver that overcorrects after regaining attentiveness by steering too strongly in her first attentive action presents a greater challenge for the agents. When choosing its next action, an agent also need to account for the driver's possible overcorrection. The amount of overcorrection is stochastic (see section~\ref{sec:driver_model}). Attentive drivers are less predictable when they overcorrect. Rather than just performing the optimal steering action, different overcorrection intensities can lead to a variety of actions at the same position. The complexity of the planning problem is higher. Generally, more searches are needed in order to evaluate a greater variety of possible states from the belief while planning.

\input{sections/plots/sims_over_plot}

\input{sections/tables/terminal_over}  

As it can be seen in Figure~\ref{fig:searches_over}, the agent using preferred actions converges sufficiently earlier towards a good policy than the other two agents. This is mainly caused by a high number of terminal states reached during evaluation runs of the agents with full and reduced action spaces (see Table~\ref{tab:over_terminal}). For example, with 750 searches, the agent using preferred actions receives an average cumulative reward of 883.43 with only 10 runs ending in a terminal state, while the other agents each reach terminal states in 42 runs. The agent with an unrestricted action space yields a return of 464.89, and the one with a restricted action space gains 473.67. In contrast to the experiment with the simple driver model, the terminal states are caused almost exclusively by particle deprivation after a formerly distracted driver becomes attentive again and overcorrects. If the agent did not account for this possibility properly, no matching node for the observation resulting from the overcorrection can be found in the agent's planning tree. In this case, the agent cannot recover and continues using uniformly random action selection (see section~\ref{sec:pomcp}), which usually leads to a terminal state after just a few actions. The agent with preferred actions is able to form an accurate belief of the environment's true state using fewer searches than the other two agents.

% TODO:
% - How often does the planner lose track and use random action selection? 
% - Why is the reduced action space driver not better than the full one? This makes zero sense. But I have to explain it somehow
% - Mention convergence behavior explicitly

\subsection{Steering overcorrection and noise}

The noise added to the driver's actions is added with the goal to make the driving more realistic, and thereby also more difficult to plan with. However, the performance of the agents in the experiment with over correction and noise is very similar to the experiment with overcorrection but without noise. Surprisingly, all agents even receive slightly higher average cumulative rewards and show a similar convergence behavior. The agent using preferred actions converges to a reward of roughly 960 with 1000 searches or more. The agent with a full action range reaches peak performance at 5000 searches with a return of about 860, staying at roughly the same level subsequently. The agent with the small action space converges at around 960, with 7500 searches and more. The actual convergence probably occurs with somewhere between 5000 and 7500 searches but no experiment was conducted with a number of searches in between.

Table~\ref{tab:noise_terminal} shows that the number of terminal states reached during experiment runs are comparable for the two agents with unweighted actions. For the agent using preferred actions, the number of terminal states reached at 750 searches is twice as high. In most of these cases, the cause for this is a combination of a strong overcorrection with a high driver action noise. This combination is unlikely but possible with the random nature of the process. Despite this deviation, the data is very similar to the experiment without noise.

\input{sections/plots/sims_noise_plot}

\input{sections/tables/terminal_noise}

% TODO: Add plot with all three results for the noise scenario for comparison?

\section{Mean lane centeredness}
\label{sec:centeredness}

% TODO: Add ground thuth (lane centeredness of the optimal strategy)

The reward reflects how well the agents manage to keep the car centered in lane and angled to the road trajectory. From the last section, it is clear that the number of simulations has a strong impact on the agents' performance. However, the last section only showcased this based on the average cumulative rewards. Figure \ref{fig:lane_centeredness} shows the average absolute lane centeredness (distance to the lane center, no matter if left or right of lane center) at every time step in the last experiment with driver action noise and a driver that oversteers after regaining attentiveness. The runs ending in terminal states are taken into account until the terminal state occurs. For the remaining actions, they are ignored in the graphs. It is therefore important to consider them (see table \ref{tab:noise_terminal}). 

With just 200 searches, most runs end in terminal states. What is striking is that the average lane centeredness is quite volatile and often drifts off into the extreme. Neither of the three agents is consistently capable of keeping the car centered in the lane. The graph for 500 searches already suggests an improvement. There are less extreme values and the standard errors are lower. The agents with unweighted actions appear to perform better than the one with preferred actions at first glance. However, still almost all runs of the two agents with unweighted actions, and only about half of the runs of the agent with preferred actions lead to terminal states. The performance of the agent with preferred actions is arguably better. Using 1000 searches marks the start of convergence for the agent with preferred actions. The average lane centeredness is consistently lower than with 500 searches throughout the experiment. The agent using a reduced set of actions performs better than with 500 searches, leading to less variation in the lane centeredness and fewer terminal states reached during the experiment. The lane centering of the agent with the full action range is virtually unchanged. When 10000 searches are performed during planning, all agents have converged to their peak performance.The two agents using unweighted actions agents achieve similar results in this case. The agent with preferred actions slightly outperforms the others but appears to have a higher variance in its lane centeredness.


% TODO:
% angular jerkiness
% The angular jerkiness is defined as the standard deviation in the change in steering action.
% mean road angle

\input{sections/plots/lane_center_plot_2}

% \section{Preservation of autonomy for an attentive driver}
% \label{sec:autonomy}

% The 

% \input{sections/tables/driver_autonomy}




