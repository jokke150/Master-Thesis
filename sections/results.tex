\chapter{Results}
\label{ch:results}

This chapter presents the results from the experiments. First, in section \ref{sec:perf_bounds} the performance of the driver without assistance, and of an agent that acts optimally, are established as the lower and upper performance bounds respectively. In section \ref{sec:grid_search}, the results of the hyperparameter optimization are presented. Then, the influence of the number of searches during planning  on the performance is evaluated in section \ref{sec:convergence}, with the aim of identifying the point of convergence for each agent. The cumulative reward but also the terminal states reached during experiments are taken into account. The results are grouped according to the different versions of the driver model. Lastly, in section \ref{sec:centeredness} the progression of the mean lane centeredness is shown for an increasing number of searches.

\section{Lower and upper performance bound}
\label{sec:perf_bounds}

The benchmark for the performance of the agents is the performance of the driver without assistance system as the lower bound, and the performance of an agent that always reacts optimally to the driver's actions as the upper bound. For both baselines, 50 runs with up to 1,000 actions each, if no terminal state is reached earlier, are performed for each of the three driver models. 

In the case of the independent driver, no run was completed successfully. At some point in any run, the driver becomes distracted and fails to adjust to a change of the course of the road, leading to lane departure. Table~\ref{tab:driver_performance} shows that the more complex the driver model is, the worse its performance is. The simple driver model and the driver model with steering overcorrection lead to a few runs with a relatively high number of successful actions before a lane departure occurs. After becoming distracted, the driver only repeats its last attentive action. In some cases, this means that the distracted driver continues to steer straight, which is less likely to lead to lane departure on the highway track than consecutively steering left or right.

The agent reacting optimally to the driver's actions has full knowledge about the car state, as well as the driver's next action. It always chooses the action that leads to the best possible combined action. It finishes every run successfully and leads to average cumulative rewards of $999.3$ for all driver models.

\input{sections/tables/driver_performance}

\section{Hyperparameter optimization}
\label{sec:grid_search}

Hyperparameter optimization is performed for agents with all three action configurations and the driver model with steering overcorrection but without noise. Grid search is used to search for the combination of search horizon and exploration constant that leads to the highest average cumulative reward after 20 runs, with up to 1,000 actions each and 1,000 searches per planning step.

\input{sections/plots/grid_plot}

\noindent
For the agent with a full, unweighted action space, two combinations lead to a sufficiently higher average cumulative reward than the others: The combination of a search horizon of 5 actions with an exploration constant of 0.75 leads to a reward of 587.67, and the combination of planning ten actions ahead with an exploration constant of 5 results in a reward of 581.40. The shallower the search horizon, the less planning time is needed at every planning step as fewer actions need to be simulated. Thus, at the same performance, a lower search horizon is preferable. The combination of a search horizon of 5 actions and an exploration constant of 0.75 is used for further experiments.

In the case of the agent restricted to using a subset of the action space, the combination of a search horizon of 5 actions and an exploration constant of 25 yields the highest average cumulative reward of 646.83. Only the combination of a search horizon of 25 actions with an exploration constant of 1.5 comes relatively close with a reward of 616.47. As a lower search horizon is preferable, the setup for the further evaluation for this agent is a search horizon of 5 actions with an exploration constant of 25. 

The best combination of search horizon and exploration constant for the agent with preferred actions is 25 actions and 1.5, respectively. This combination yields an average cumulative reward of 942.05, which is sufficiently higher than the return of any other combination. Consequently, this is the combination used for this agent in subsequent experiments.

\section{Performance impact of the number of searches during planning}
\label{sec:convergence}

We want to determine after how many searches during planning the lane-keeping performance of the three evaluated agents converges. Performing more searches means evaluating more possible scenarios. In turn, more evaluated scenarios enable the agents to form better policies. However, after some number of searches, the information gain from additional searches decreases, and performance is expected to converge \parencite{pomcp}. We evaluate each of the agents in the scenarios with the simple driver model, the driver model with steering overcorrection after regaining attentiveness, and the driver with overcorrection and noise. A lower number of searches is preferable as each search requires a substantial amount of planning time. The applicability of a planner is limited by the planning time required for good results. Thus, knowing after how many searches the performance converges, and therefore being able to choose the minimal number of searches to perform for a good result, is critical. 

% The agent's challenge is threefold: First, it must accurately determine where the car is located on the track. Second, it needs to estimate whether the driver is attentive or not. Third, it must decide on an appropriate action choice, taking into account future consequences of the chosen action. 

\subsection{Simple driver model}

Figure \ref{fig:conv_norm} illustrates the performance differences between evaluations with various different numbers of searches during planning for the experiment with the simple driver model. All evaluated agents lead to similar convergence behavior. Already with just 200 searches, the average cumulative reward is above 600 for all agents. However, as it can be seen in Table~\ref{tab:simple_terminal}, the number of runs that lead to a terminal state is high. In the runs resulting in a terminal state, the agents are able to assist the drivers well at first but suffer from particle deprivation after some time (see section \ref{sec:particle_deprivation}). Then, their belief deviates noticeably from the true states of the car and the driver. The agents are not able anymore to make accurate assumptions about whether the driver is attentive or not. Thus, they are rendered unable to decide on the right actions to keep the car centered.

\input{sections/plots/sims_norm_plot}

The agent using the full range of actions reaches four terminal states with 300 searches, whereas the other two agents need more searches to perform well. The best result for all three agents is achieved with 1,500 searches. Then, no run results in a terminal state. The agent with the full action space receives an average cumulative reward of 957.83, the agent with a reduced action space yields 981.99, and the agent using preferred actions gains 973.88. 

For more searches, the average cumulative rewards are similar for the agents with a subset of actions and preferred actions. They seem to have converged. In contrast, the agent with a complete action space encounters four terminal states in the trial with 5000 searches and six in the experiment with 7,500 searches. These terminal states are reached early on during the run - executing less than 50 actions before. They are caused by an extreme action of the agent that leads to the opposite of optimal steering behavior. The actions completely overrule the driver's actions, who is even concentrated during three of the ten occasions. The reason for choosing these extreme actions is a poor belief that strongly deviates from the true states of the environment and the driver. The agent with preferred actions is less likely to perform extreme actions, and the agent with a subset of minor actions cannot do so. 

The agent restricted to use only a subset of minor actions reaches terminal states in two states in the experiments with 2,500, 7,500, and 10,000 searches. These are not caused by particle deprivation. In all cases, the car is in a road bend, and the driver is distracted, steering in the wrong direction. The agent performs its best possible action by steering as much as possible in the opposite direction than the driver. However, because the agent's range of actions is severely limited, the combination of the agent's and driver's actions is not enough to keep the car in the lane. 

Using preferred actions results in only one terminal state for experiments with 1,500 searches or more. The reason, like for the terminal states reached by the agent without weighted actions, is particle deprivation. There are multiple occasions where a distracted driver steers in the wrong direction in a road bend, and the agent is able to correct the steering by effectively reversing the driver's actions.

\input{sections/tables/terminal_norm}

Table \ref{tab:simple_terminal} also includes the average planning times the agents require to perform all their searches in each planning episode. The planning times are mostly influenced by the search horizon. Whereas the agents without preferred actions both only look five actions ahead during planning, the agent with preferred actions considers 25 possible future actions in its tree. Therefore, it needs to perform five times the number of simulations during planning, which requires more time. The planning time is needed before executing each action. In the most extreme case, with 10,000 searches, this means that the agent with preferred actions needs to plan for an average of 22.36 seconds of real-time before deciding on the action for the next 0.1 seconds of simulated driving time. The simulation halts during planning. However, in a real-world scenario, this is not feasible.

\subsection{Steering overcorrection}

A driver that overcorrects after regaining attentiveness by steering too strongly in her first attentive action presents a greater challenge for the agents. When choosing its next action, an agent also needs to account for the driver's possible overcorrection. The amount of overcorrection is stochastic (see section~\ref{sec:driver_model}). Attentive drivers are less predictable when they overcorrect. Rather than just performing the optimal steering action, different overcorrection intensities can lead to a variety of actions at the same position. The complexity of the planning problem is higher. Generally, more searches are needed in order to evaluate a greater variety of possible states from the belief while planning.

\input{sections/plots/sims_over_plot}

As it can be seen in Figure~\ref{fig:searches_over}, the agent using preferred actions converges sufficiently earlier towards a good policy than the other two agents. This is mainly caused by a high number of terminal states reached during evaluation runs of the agents without preferred actions (see Table~\ref{tab:over_terminal}). For example, with 750 searches, the agent using preferred actions receives an average cumulative reward of 883.43 with only ten runs ending in a terminal state, while the other agents each reach terminal states in 42 runs. The agent with an unrestricted action space yields a mean return of 464.89, and the one with a restricted action space gains 473.67. 

\input{sections/tables/terminal_over}  

In contrast to the experiment with the simple driver model, the terminal states are caused almost exclusively by particle deprivation after a formerly distracted driver becomes attentive again and overcorrects. If the agent did not account for this possibility properly, no matching node for the observation resulting from the overcorrection is contained in the agent's planning tree. In this case, the agent cannot recover and continues using uniformly random action selection (see section~\ref{sec:pomcp}), which usually leads to a terminal state after just a few actions. The agent with preferred actions is able to form an accurate belief of the environment's true state using fewer searches than the other two agents.

Moreover, table \ref{tab:over_terminal} presents the average planning times needed by the agents. These are comparable with the results for the scenario with the simple driver model.

\subsection{Steering overcorrection and noise}

The noise added to the driver's actions is added with the goal to make the driver's behavior more realistic and thereby also more difficult to plan with. However, the performance of the agents in the experiment with overcorrection and noise is very similar to the experiment with overcorrection but without noise. Figure \ref{fig:conv_noise} displays the performance differences for the agents with various numbers of searches. Surprisingly, all agents even receive slightly higher average cumulative rewards and show a similar convergence behavior. The agent using preferred actions converges to a reward of roughly 960 with 1,000 searches or more. The agent with a full action range reaches peak performance at 5,000 searches with a return of about 860, staying at roughly the same level subsequently. The agent with the small action space converges at around 960, with 7,500 searches and more. The actual convergence probably occurs with somewhere between 5,000 and 7,500 searches, but no experiment was conducted with a number of searches in between.

\input{sections/plots/sims_noise_plot}

\noindent
Table~\ref{tab:noise_terminal} shows the number of terminal states reached during the experiments and the average planning times needed. The numbers of terminal runs are comparable for the two agents with unweighted actions. For the agent using preferred actions, the number of terminal states reached at 750 searches is twice as high. In most of these cases, the cause for this is a combination of a strong overcorrection with a high driver action noise. This combination is unlikely but possible with the random nature of the process. Despite this deviation, the data is very similar to the experiment without noise. The average planning times are similar to the times from the experiments with the other two driver models.

\input{sections/tables/terminal_noise}

\section{Mean lane centeredness}
\label{sec:centeredness}

The reward reflects how well the agents manage to keep the car centered in the lane and angled to the road trajectory. From the last section, it is clear that the number of simulations has a strong impact on the agents' performance. However, the last section only showcased this based on the average cumulative rewards and terminal states reached. In this section, we want to emphasize the difference in lane centeredness that stems from using more searches during planning. More specifically, we consider the mean absolute lane centeredness, which is the average distance to the lane center, regardless of whether the deviation is on the left or right side.

For the experiment with the driver model with driver action noise and steering overcorrection, figure \ref{fig:lane_centeredness} shows the average absolute lane centeredness for an increasing number of searches. We focus on this figure in the analysis. The results for the other two scenarios are similar (see appendix \ref{sec:lane-center-app-1} and appendix \ref{sec:lane-center-app-2}). The runs ending in terminal states are taken into account until the terminal state occurs. For the remaining actions, they are ignored in the graphs. It is therefore important to consider them (see table \ref{tab:noise_terminal}). 

With just 200 searches, most runs end in terminal states. What is striking is that the average lane centeredness is quite volatile and often drifts off into the extreme. Neither of the three agents is consistently capable of keeping the car centered in the lane. The graph for 500 searches already suggests an improvement. There are less extreme values, and the standard errors are lower. The agents with unweighted actions appear to perform better than the agent with preferred actions at first glance. However, still, almost all runs of the two agents with unweighted actions lead to terminal states, and only about half of the runs of the agent with preferred actions. The performance of the agent with preferred actions is arguably better. Using 1,000 searches marks the start of convergence for the agent with preferred actions. The average lane centeredness is consistently lower than with 500 searches throughout the experiment. The agent using a reduced set of actions performs better than with 500 searches, leading to less variation in the lane centeredness and fewer terminal states reached during the experiment. The lane centering of the agent with the full action range is virtually unchanged. When 10,000 searches are performed during planning, all agents have converged to their peak performance. The two agents using unweighted actions agents achieve similar results in this case. The agent with preferred actions slightly outperforms the others but appears to have a higher variance in its lane centeredness.

\input{sections/plots/lane_center_plot_2}



