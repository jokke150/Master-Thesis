\chapter{Conclusion and future outlook}
\label{ch:conclusion}

% reviews the challenges, insights, and constributions
This chapter concludes the study of this thesis about a distraction-aware lane-keeping assistance with a human in the loop. Section \ref{sec:conclusion} presents the conclusions of this thesis. Section \ref{sec:future} outlines the necessary adaptations required to apply our approach with a more complex driver model or a human driver.

\section{Conclusion}
\label{sec:conclusion}

% Main challenge 

% Has it been successful?

In this thesis, an agent acting as a lane-keeping assistance system is conceived that estimates the driver's distraction online, sharing control over the car with the driver. The agent does not observe whether the driver is attentive or distracted. For its assessment of the driver's distraction, the agent relies solely on observations about driver performance measures, such as the steering actions of the driver, and the position of the car. The problem is modeled as a POMDP to account for the uncertainty about the driver's distraction and the exact positon of the car. The POMCP algorithm is applied to solve the POMDP online. To the best of our knowledge, this is the first work that applies online POMDP solving to address the uncertainty about the driver's distraction in a shared control lane keeping scenario, while relying only on commonly available driver performance measures as observations.

\vspace{1em}
\noindent
The main conclusions of this thesis can be summarized as follows:
\begin{enumerate}
    \item We provide a POMDP model with continuous state as a representation of the shared control lane-keeping scenario, outlining how both the human driver and the car's dynamic can be simulated. 
    \item Our model for the human driver is simple. However, our modeling approach is also suitable for the integration of a more sophisticated driver model (see section \ref{sec:complex-driver}).
    \item We enable an agent to act as a lane keeping assistant to the driver, taking into account the driver's potential distraction. The POMDP is solved online by applying the POMCP algorithm. Expermental results show that the driving performance is enhanced.
    \item Particle deprivation is a common problem with a particle filter approach such as POMCP. Implementing particle injection (see section \ref{sec:particle_deprivation}) and introducing domain knowledge by the use of preferred actions (see section \ref{sec:preferred_actions}) leads to an imporovement.
    \item Using the TORCS driving simulator for forward simulation during planning with POMCP is not efficient enough for a real-time scenario. The performance needs to be significantly optimized. Suggestions on how to achieve this are provided in section \ref{sec:perf_opt}.
    \item The lane-keeping performance of our approach is not up to par with traditional lane keeping assistance systems. The autonomy of the driver is not significantly increased by estimating the driver's distraction. The immediate application of the approach is not advisable. Section \ref{sec:future} outlines opportunities for improvement.
    \item Our simulation method allows for repetition of experiments. Problems can be revisited and analyzed. This is important in the safety-critical domain of automated driving.
\end{enumerate}

%%%%%%
% Research questions

% How can a lane-keeping scenario with shared control between a potentially distracted human driver and an agent be modeled using a POMDP?

% Can the agent estimate the driver's distraction using driver performance measures alone, allowing it to take appropriate actions when the driver becomes distracted?

% Is the solution approach viable for a real-world scenario, and if not, what limitations are there, and what are potential methods to solve them?




% The agent should have access to the full action space. Otherwise, it is not prepared for some situations.

%%%%
% Agent acts while driver is attentive and acting optimally

% Add penalty to reward to reduce the occurences of intervening while the driver is attentive?
% Additional clues about the driver's attentiveness are needed. 

\section{Road toward application with human drivers}

In this section, adaptations and improvements that we suggest for the application of our approach with a more complex driver model or a human driver are discussed. 

\label{sec:future}
\subsection{Avoiding particle deprivation}

The main problem encountered during planning is particle deprivation. This happens when the belief of the agent diverges substantially from the true state. In our case, this is mostly caused in the forward search performed during the planning process by taking into account either too few possible future trajectories, or not successfully identifying and focusing on more probable ones. Taking into account too few possibilities can be circumvented by increasing the exploration. However, this has a negative performance impact. A more promising solution wuld be to improve the performance of the generative model we use for the forward simualtion during the planning phase. The next section addresses this issue. Enabling the agent to focus on more prbable scenarios can be achieved in two ways: First, one can pass domain knowledge to the agent, like we did by using preferred actions. Second, offering the agent more sohisticated observations could lead to a more accurate belief state.

The domain knowledge we offer our agent, with the chosen implementation of preferred actions, is not very extensive, nor accurate. It was hand crafted on the assumption that strong steering actions are less likely to be needed. Providing better domain knowledge may improve the agent's action selection during planning substantially. For example, this could be achieved by learning an initial policy offline from driving data and providing it to the agent. 

It is possible to classify driver distraction using driver performance measures alone \parencite{dist-det-perf} . However, estimating the driver's attention using only her past steering actions, and the car's position is difficult. Including additional information may be helpful. This can be additional performance measures, such as accelleration, and braking behavior. But using more sophisticated information, such as eye gaze, the driver's head position, or even biometric factors, like her heart rate or brain activity, is conceivable. The likelihood of a good prediction increases with a stronger correlation of the obseverations with the driver's attentiveness. Nevertheless, it must be considered how likely it is to have access to this data in a realistic driving situation.

\subsection{Performance optimization}
\label{sec:perf_opt}

Our experiments have shown that a considerable planning time is required for the agent to achieve acceptable lane-keeping performance. There are two main causes for this. Firstly, the generative model we use for the Monte-Carlo forward simulation during planning is not very efficient. Secondly, the POMCP algorithm itself is not very efficient; forward simulations during planning are sequential.

The generative model uses TORCS for the simulations of the driver states. TORCS is well capable of perfoming faster than real-time simulations. However, thousands of trajectories need to be simulated. The number of performed searches, and the search depth determine the volume of simulations that are performed during the online planning. For example, with 10,000 searches, and a search horizon of 25, up to 250,000 individual simulations are performed during each online planning step before the next action can be executed. No extensive profiling has been carried out. Nevertheless, it is very likely that the performance of TORCS could be highly optimized. However, optimizing TORCS would require an extensive amount of work. It may be avoided by either replacing TORCS with a simpler driving simulator, or parallelizing the planning. A simpler model may represent the driving dynamics less accurately, but can potentially still lead to good results. Parallelization of the planning would allow to use multiple instances of TORCS simultaneously, effectively evaluating multiple possible future scenarios at once.

\cite{pomcp-parallel} show that POMCP can be parallelized by constructing multiple search trees simultaneously while planning and merging them afterwards. The planning time is reduced substantially, without strongly negatively impacting the solution quality. Another, more potent parallelized MCTS algorithm that could be considered instead of POMCP is DESPOT-$\alpha$ \parencite{despot-a}. It utilizes both CPU and GPU parallelization and could decrease the required planning time substantially, while keeping a similar performance, or even improvng it. However, it requires the observation probabilities $Z$ to be known explicitly for the POMDP (see section \ref{sec:pomdp}). It may be possible to approximate the observation probabilities. A more in-depth investigation of the necessary adaptations than has been carried out in the context of this thesis would be necessary.

\subsection{Integrating a sophisticated driver model}
\label{sec:complex-driver}
% Requirement for an accurate generative model

The POMCP algorithm is taking a model-based planning approach. The driver models we use in our experiments are simple and not suited to accurately model real human behavior. The POMCP algorithm is dependent on the accuracy of the model used for forward simulation. In our experiments, the same model is used to represent the driver during evalution, and to simulate drivers for the search tree construction during planning. With a real human as driver, this is not possible. If the agent shares control over the car with a human, a sophisticated, accurate model is required for Monte-Carlo sampling during planning. Driver behavior modeling is an active research field. It is not within the scope of this thesis to provide an overview of driver modeling approaches. Extensive reviews of different methods can be found in literatue by \cite{model-review-3}, \cite{model-review-4}, and \cite{model-review-5}.

The integration of a more advance driver model is straightforward. It can be used as a replacement of one of the three driver models we use for our experiments. The only requirement is that the agent needs to be able to provide a belief state (car state and driver model state) to the driver model, getting a driver action and the next driver model state in return. The exact composition of the driver model state can be arbitrary, as the agent never evaluates the driver model state directly but just stores it in a belief particle to be used in the next forward search.

\subsection{Continuous action and observation space}
\label{sec:conclusion-continuous}

We utilize discretization to be able to use POMCP with a naturally continuous action and observation space (see section \ref{sec:discretization}). Discretization is necessary for POMCP to work. However, it comes with the drawback of a loss of precision. Even if one would increase the resolution of the discretization - using more bins, some degree of information loss is inevitable. The alternative would be to work with the continuous spaces direclty. A switch from POMCP to another POMDP solver would be required for this.

Various solvers have been developed to work with fully continuous POMDP. Two algorithms worth mentioning are POMCPOW, an extension of POMCP using progressive widening \parencite{online_pomdp_cont}, and LABECOP, which, in contrast to POMCPOW, avoids limiting the number of observations considered during planning \parencite{online-cont-pomdp-2}. However, both methods require the observation probabilities $Z$ to be explicitly known, which is not the case in our scenario. One would need to find a way to circumvent this problem. A potential approach would be to approximate the observation probability distribution in some way but this is not further considered in this thesis.

\subsection{Use of active probing}

Human drivers likely react to the actions of the agent, with potentially different reaction times or bhavior for distracted drivers. The driver models used in our experiments did not account for this. \cite{att_intersec} show that actively probing human drivers can reduce the uncertainty about their mental state. Enabling the agent to gather information about the human by actively probing it might help to imptove the agent's performance. Therefore, in a scenario with a human driver, in which the internal state of the human is very complex and therefore hard to estimate, active probing, like intentional small steering actions, even if they are not necessary, could be used to gauge the state of the driver more accurately.

%%%%%%
% Step further: Active probing 
% Although efficient, these approximations sacrifice animportant aspect of POMDPs: the ability toactively gatherinformation.

% Intersection, probing other cars: \cite{att_intersec}
% Planning for cars that coordinate with people: leveraging effects onhuman actions for planning and active information gathering overhuman internal state
% Information Gathering Actions over Human Internal State
% -> Our key insight is that robots can leverage their own actions to help estimation of human internalstate.\

%%%%%%
% Step further: Bi-directional feedbackk

% \cite{hitl_pomdp} uses feedback
% Sharing Control With Haptics: Seamless Driver Support From Manual to Automatic Control