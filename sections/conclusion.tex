\chapter{Conclusion and future outlook}
\label{sec:conclusion}

\section{Conclusion}

\section{Road toward application with human drivers}
\subsection{Performance optimization}
% Currently, the whole torcs state is used as state in the belief. A lot of data in it may not be relevant for planning or rarely change. This comes with a huge memory overhead. Optimizing the state representation by only including what is really needed will improve the memory requirements. Currently, the experiments with 10.000 searches require about 8 GB of memory each. 
\subsection{Integrating realistic driver and environment models}
\subsection{Continuous action and observation space}
\subsection{Using other POMDP solvers}
    % \item POMCP (Monte-Carlo tree search from the current belief using samples from a black-box simulator.)
    % \item DESPOT (Avoids POMCP’s extremely poor worst-case behavior by evaluating policies on a small number of sampled scenarios.)
    % \item DESPOT-IS (Importance sampling improves DESPOT’s performance when there are critical, but rare events, which are difficult to sample.)
    % \item HyP-DESPOT (Parallelization speeds up online planning by up to several hundred times, compared with the original DESPOT algorithm.)
    
    % How to get transition and observation probabilities?
    % Approximate enough?
    % Use supervised learning?
    % Use prior knowledge?