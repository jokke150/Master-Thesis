\chapter{Conclusion and future outlook}
\label{sec:conclusion}

\section{Conclusion}

% The agent should have access to the full action space. Otherwise, it is not prepared for some situations.

% Additional clues about the driver's attentiveness are needed. 

% 

\section{Road toward application with human drivers}
\subsection{Avoiding particle deprivation}
% Main problem: particle deprivation. Encountered sceanrio has not bee considered (enough) in the planning phase.
% Potential solution: Add additional clues for the agent about the attentiveness of the driver. (Look up some possibilities in papers)

\subsection{Performance optimization}
\label{sec:perf_opt}
% Currently, the whole torcs state is used as state in the belief. A lot of data in it may not be relevant for planning or rarely change. This comes with a huge memory overhead. Optimizing the state representation by only including what is really needed will improve the memory requirements. Currently, the experiments with 10.000 searches require about 8 GB of memory each. 


% First, the generative model can be made more efficient. Second, the POMCP algorithm could be parallelized or potentially even replaced by a more efficient algorithm.

% See Parallelizing POMCP to Solve Complex POMDPs
% See HypDespot

\subsection{Integrating realistic driver and environment models}
% Requirement for an accurate generative model
\subsection{Continuous action and observation space}
\subsection{Using other POMDP solvers}
    % \item POMCP (Monte-Carlo tree search from the current belief using samples from a black-box simulator.)
    % \item DESPOT (Avoids POMCP’s extremely poor worst-case behavior by evaluating policies on a small number of sampled scenarios.)
    % \item DESPOT-IS (Importance sampling improves DESPOT’s performance when there are critical, but rare events, which are difficult to sample.)
    % \item HyP-DESPOT (Parallelization speeds up online planning by up to several hundred times, compared with the original DESPOT algorithm.)
    
    % How to get transition and observation probabilities?
    % Approximate enough?
    % Use supervised learning?
    % Use prior knowledge?