\chapter{Conclusion and future outlook}
\label{ch:conclusion}

% reviews the challenges, insights, and constributions
This chapter concludes the study of this thesis about a distraction-aware lane-keeping assistance with a human in the loop. Section \ref{sec:conclusion} presents the conclusions of this thesis. Section \ref{sec:future} outlines the necessary adaptations required to apply our approach with a more complex driver model or a human driver.

\section{Conclusion}
\label{sec:conclusion}

% Main challenge 

% Has it been successful?

In this thesis, an agent acting as a lane-keeping assistance system is conceived that estimates the driver's distraction online, sharing control over the car with the driver. The agent does not observe whether the driver is attentive or distracted. For its assessment of the driver's distraction, the agent relies solely on observations about driver performance measures, such as the steering actions of the driver, and the position of the car. The problem is modeled as a POMDP to account for the uncertainty about the driver's distraction and the exact positon of the car. The POMCP algorithm is applied to solve the POMDP online. To the best of our knowledge, this is the first work that applies online POMDP solving to address the uncertainty about the driver's distraction in a shared control lane keeping scenario, while relying only on commonly available driver performance measures as observations.

\vspace{1em}
\noindent
The main conclusions of this thesis can be summarized as follows:
\begin{enumerate}
    \item We provide a POMDP model with continuous state as a representation of the shared control lane-keeping scenario, outlining how both the human driver and the car's dynamic can be simulated. 
    \item Our model for the human driver is simple. However, our modeling approach is also suitable for the integration of a more sophisticated driver model (see Section \ref{sec:complex-driver}).
    \item We enable an agent to act as a lane keeping assistant to the driver, taking into account the driver's potential distraction. The POMDP is solved online by applying the POMCP algorithm. Expermental results show that the driving performance is enhanced.
    \item Particle deprivation is a common problem with a particle filter approach such as POMCP. Implementing particle injection (see Section \ref{sec:particle_deprivation}) and introducing domain knowledge by the use of preferred actions (see Section \ref{sec:preferred_actions}) leads to an imporovement.
    \item Using the TORCS driving simulator for forward simulation during planning with POMCP is not efficient enough for a real-time scenario. The performance needs to be significantly optimized. Suggestions on how to achieve this are provided in Section \ref{sec:perf_opt}.
    \item The lane-keeping performance of our approach is not up to par with traditional lane keeping assistance systems. The autonomy of the driver is not significantly increased by estimating the driver's distraction. The immediate application of the approach is not advisable. Section \ref{sec:future} outlines opportunities for improvement.
    \item Our simulation method allows for repetition of experiments. Problems can be revisited and analyzed. This is important in the safety-critical domain of automated driving.
\end{enumerate}

%%%%%%
% Research questions

% How can a lane-keeping scenario with shared control between a potentially distracted human driver and an agent be modeled using a POMDP?

% Can the agent estimate the driver's distraction using driver performance measures alone, allowing it to take appropriate actions when the driver becomes distracted?

% Is the solution approach viable for a real-world scenario, and if not, what limitations are there, and what are potential methods to solve them?




% The agent should have access to the full action space. Otherwise, it is not prepared for some situations.

%%%%
% Agent acts while driver is attentive and acting optimally

% Add penalty to reward to reduce the occurences of intervening while the driver is attentive?
% Additional clues about the driver's attentiveness are needed. 

\section{Road toward application with human drivers}

In this section, adaptations and improvements that we suggest for the application of our approach with a more complex driver model or a human driver are discussed. 

\label{sec:future}
\subsection{Avoiding particle deprivation}

The main problem encountered during planning is particle deprivation. This happens when the belief of the agent diverges substantially from the true state. In our case, this is mostly caused in the forward search performed during the planning process by taking into account either too few possible future trajectories, or not successfully identifying and focusing on more probable ones. Taking into account too few possibilities can be circumvented by increasing the exploration. However, this has a negative performance impact. A more promising solution wuld be to improve the performance of the generative model we use for the forward simualtion during the planning phase. The next section addresses this issue. Enabling the agent to focus on more prbable scenarios can be achieved in two ways: First, one can pass domain knowledge to the agent, like we did by using preferred actions. Second, offering the agent more sohisticated observations could lead to a more accurate belief state.

The domain knowledge we offer our agent, with the chosen implementation of preferred actions, is not very extensive, nor accurate. It was hand crafted on the assumption that strong steering actions are less likely to be needed. Providing better domain knowledge may improve the agent's action selection during planning substantially. For example, this could be achieved by learning an initial policy offline from driving data and providing it to the agent. 

It is possible to classify driver distraction using driver performance measures alone \parencite{dist-det-perf} . However, estimating the driver's attention using only her past steering actions, and the car's position is difficult. Including additional information may be helpful. This can be additional performance measures, such as accelleration, and braking behavior. But using more sophisticated information, such as eye gaze, the driver's head position, or even biometric factors, like her heart rate or brain activity, is conceivable. The likelihood of a good prediction increases with a stronger correlation of the obseverations with the driver's attentiveness. Nevertheless, it must be considered how likely it is to have access to this data in a realistic driving situation.

\subsection{Performance optimization}
\label{sec:perf_opt}

Our experiments have shown that a considerable planning time is required for the agent to achieve acceptable lane-keeping performance. There are two main causes for this. Firstly, the generative model we use for the Monte-Carlo forward simulation during planning is not very efficient. Secondly, the POMCP algorithm itself is not very efficient; forward simulations during planning are sequential.

The generative model uses TORCS for the simulations of the driver states. TORCS is well capable of perfoming faster than real-time simulations. However, thousands of trajectories need to be simulated. The number of performed searches, and the search depth determine the volume of simulations that are performed during the online planning. For example, with 10,000 searches, and a search horizon of 25, up to 250,000 individual simulations are performed during each online planning step before the next action can be executed. No extensive profiling has been carried out. Nevertheless, it is very likely that the performance of TORCS could be highly optimized. However, optimizing TORCS would require an extensive amount of work. It may be avoided by either replacing TORCS with a simpler driving simulator, or parallelizing the planning. A simpler model may represent the driving dynamics less accurately, but can potentially still lead to good results. Parallelization of the planning would allow to use multiple instances of TORCS simultaneously, effectively evaluating multiple possible future scenarios at once.

\cite{pomcp-parallel} show that POMCP can be parallelized by constructing multiple search trees simultaneously while planning and merging them afterwards. The planning time is reduced substantially, without strongly negatively impacting the solution quality. Another, more potent parallelized MCTS algorithm that could be considered instead of POMCP is DESPOT-$\alpha$ \parencite{despot-a}. It utilizes both CPU and GPU parallelization and could decrease the required planning time substantially, while keeping a similar performance, or even improvng it. However, it requires the observation probabilities $Z$ to be known explicitly for the POMDP (see Section \ref{sec:pomdp}). It may be possible to approximate the observation probabilities. A more in-depth investigation of the necessary adaptations than has been carried out in the context of this thesis would be necessary.

\subsection{Integrating a sophisticated driver model}
\label{sec:complex-driver}
% Requirement for an accurate generative model



\subsection{Continuous action and observation space}
\label{sec:conclusion-continuous}



% TODO: Mention? Speaks for bad experiment design
% \subsection{Using other POMDP solvers}

% \item POMCP (Monte-Carlo tree search from the current belief using samples from a black-box simulator.)
% \item DESPOT (Avoids POMCP’s extremely poor worst-case behavior by evaluating policies on a small number of sampled scenarios.)
% \item DESPOT-IS (Importance sampling improves DESPOT’s performance when there are critical, but rare events, which are difficult to sample.)
% \item HyP-DESPOT (Parallelization speeds up online planning by up to several hundred times, compared with the original DESPOT algorithm.)

% How to get transition and observation probabilities?
% Approximate enough?
% Use supervised learning?
% Use prior knowledge?

\subsection{Use of active probing}

%%%%%%
% Step further: Active probing 
% Although efficient, these approximations sacrifice animportant aspect of POMDPs: the ability toactively gatherinformation.

% Intersection, probing other cars: \cite{att_intersec}
% Planning for cars that coordinate with people: leveraging effects onhuman actions for planning and active information gathering overhuman internal state
% Information Gathering Actions over Human Internal State
% -> Our key insight is that robots can leverage their own actions to help estimation of human internalstate.\

%%%%%%
% Step further: Bi-directional feedback

% \cite{hitl_pomdp} uses feedback
% Sharing Control With Haptics: Seamless Driver Support From Manual to Automatic Control