\chapter{Conclusion and future outlook}
\label{ch:conclusion}

% reviews the challenges, insights, and constributions
This chapter concludes the study of this thesis about distraction-aware lane-keeping assistance with a human in the loop. Section \ref{sec:conclusion} presents the conclusions of this thesis. Section \ref{sec:future} outlines the necessary adaptations required to apply our approach with a more complex driver model or a human driver.

\section{Conclusion}
\label{sec:conclusion}

% Main challenge 

% Has it been successful?

In this thesis, an agent acting as a lane-keeping assistance system is conceived that estimates the driver's distraction online, sharing control over the car with the driver. The agent does not observe whether the driver is attentive or distracted. For its assessment of the driver's distraction, the agent relies solely on observations about driving performance measures, such as the steering actions of the driver, and sensory information about the position of the car. The problem is modeled as a POMDP to account for the uncertainty about the driver's distraction and the exact positon of the car. The POMCP algorithm \parencite{pomcp} is applied to solve the POMDP online. To the best of our knowledge, this is the first work that applies online POMDP solving to address the uncertainty about a driver's distraction in a shared control lane-keeping scenario while relying only on commonly available measures as observations.

\vspace{1em}
\noindent
The main conclusions of this thesis can be summarized as follows:
\begin{enumerate}
    \item We provide a POMDP model with a continuous state as a representation of the shared control lane-keeping scenario, outlining how both the human driver and the car's dynamic can be simulated. 
    \item Our model for the human driver is simple. However, our modeling approach is also suitable for the integration of a more sophisticated driver model (see section \ref{sec:complex-driver}).
    \item We enable an agent to act as a lane-keeping assistant to the driver, taking into account the driver's potential distraction. The POMDP is solved online by applying the POMCP algorithm. Experimental results show that the driving performance is enhanced.
    \item Particle deprivation is a common problem with a particle filter approach such as POMCP. Implementing particle injection (see section \ref{sec:particle_deprivation}) and introducing domain knowledge by the use of preferred actions (see section \ref{sec:preferred_actions}) leads to an improvement.
    \item Using the TORCS driving simulator as a generative model during planning with POMCP is not efficient enough for a real-time scenario. The performance needs to be significantly optimized. Suggestions on how to achieve this are provided in section \ref{sec:perf_opt}.
    \item The lane-keeping performance of our approach is inferior to traditional lane-keeping assistance systems. The autonomy of the driver is not significantly increased by estimating the driver's distraction. The immediate application of the approach is not advisable. Section \ref{sec:future} outlines opportunities for improvement.
    \item Our simulation method allows for a repetition of experiments. Problems can be revisited and analyzed. This is important in the safety-critical domain of automated driving.
\end{enumerate}

%%%%%%
% Research questions

% How can a lane-keeping scenario with shared control between a potentially distracted human driver and an agent be modeled using a POMDP?

% Can the agent estimate the driver's distraction using driver performance measures alone, allowing it to take appropriate actions when the driver becomes distracted?

% Is the solution approach viable for a real-world scenario, and if not, what limitations are there, and what are potential methods to solve them?




% The agent should have access to the full action space. Otherwise, it is not prepared for some situations.

%%%%
% Agent acts while driver is attentive and acting optimally

% Add penalty to reward to reduce the occurences of intervening while the driver is attentive?
% Additional clues about the driver's attentiveness are needed. 

\section{Road toward application with human drivers}
\label{sec:future}

In this section, adaptations and improvements that we suggest for the application of our approach with a more complex driver model or a human driver are discussed. 

\subsection{Avoiding particle deprivation}

The main problem encountered during planning is particle deprivation. This happens when the belief of the agent diverges substantially from the true state. In our case, this is mostly caused by either taking into account too few possible future trajectories during planning or not successfully identifying and focusing on more probable future scenarios. Taking into account too few possibilities can be circumvented by increasing the exploration. However, this has a negative performance impact. A more promising solution would be to improve the performance of the generative model we use for the search during the planning phase. The next section addresses this issue. Enabling the agent to focus on more probable scenarios can be achieved in two ways: First, one can pass domain knowledge to the agent, as we did by using preferred actions. Second, offering the agent more sophisticated observations could lead to a more accurate belief state.

The domain knowledge we offer our agent, with the chosen implementation of preferred actions, is not very extensive nor accurate. It is based on the assumption that strong steering actions are less likely to be needed. Providing better domain knowledge may improve the agent's action selection during planning substantially. For example, this could be achieved by learning an initial policy offline from driving data and providing it to the agent (see \cite{combining_on_offline}).  

It is possible to classify driver distraction using driver performance measures alone \parencite{dist-det-perf}. However, estimating the driver's attention using only her past steering actions and the car's position is difficult. Including additional information may be helpful. This could be additional performance measures, such as acceleration and braking behavior. Additionally, using more sophisticated information is conceivable, such as driver's eye gaze, head position, or even biometric factors, like her heart rate or brain activity. The likelihood of a good prediction increases with a stronger correlation of the observations with the driver's attentiveness. Nevertheless, it must be considered how likely it is to have access to this data in a realistic driving situation.

\subsection{Performance optimization}
\label{sec:perf_opt}

Our experiments have shown that a considerable planning time is required for the agent to achieve acceptable lane-keeping performance. There are two main causes for this. Firstly, the generative model we use for the Monte-Carlo forward simulation during planning is not very efficient. Secondly, the POMCP algorithm itself is not very efficient; forward simulations during planning are sequential.

The generative model uses TORCS for the simulations of the driver states. Thousands of trajectories need to be simulated. The number of performed searches and the search depth determine the volume of simulations performed during the online planning. For example, with 10,000 searches and a search horizon of 25, up to 250,000 individual simulations are performed during each online planning step before the next action can be executed (less than 250.000 if terminal states are reached before the search horizon). We did not perform any sophisticated profiling. Nevertheless, it is very likely that the performance of TORCS could be highly optimized. However, optimizing TORCS would require an extensive amount of work. This may be avoided by either replacing TORCS with a simpler, more efficient driving simulator or by parallelizing the planning. A simpler model may represent the driving dynamics less accurately but can potentially still lead to good results (see \cite{hitl_pomdp} for an example of a simple mathematical model). Parallelization of the planning would enable the use of multiple instances of TORCS simultaneously. Thereby, multiple possible future scenarios can effectively be evaluated at once.

\cite{pomcp-parallel} show that POMCP can be parallelized by constructing multiple search trees simultaneously during the planning phase and merging them afterward. The planning time is reduced without strongly negatively impacting the solution quality. Another, more potent parallelized MCTS algorithm that could be considered instead of POMCP is DESPOT-$\alpha$ \parencite{despot-a}. It utilizes both CPU and GPU parallelization and could decrease the required planning time substantially while keeping a similar performance or even improving it. However, it requires the observation probabilities $Z$ to be known explicitly for the POMDP (see section \ref{sec:pomdp}). It may be possible to approximate the observation probabilities. A more in-depth investigation of the necessary adaptations would be necessary.

\subsection{Integrating a sophisticated driver model}
\label{sec:complex-driver}
% Requirement for an accurate generative model

The POMCP algorithm is taking a model-based planning approach. The driver models we use in our experiments are simple and not suited to accurately model real human behavior. The POMCP algorithm is dependent on the accuracy of the model used for planning. In our experiments, the same model is used to represent the driver during evaluation and to simulate drivers for the search tree construction during planning. With a real human driver, this is not possible. If the agent shares control over the car with a human, a sophisticated, accurate model is required for Monte-Carlo sampling during planning. Driver behavior modeling is an active research field. It is not within the scope of this thesis to provide an overview of driver modeling approaches. Extensive reviews of different methods can be found in literature by \cite{model-review-3}, \cite{model-review-4}, and \cite{model-review-5}.

The integration of a more advanced driver model is straightforward. It can be used as a replacement for one of the three driver models we use in our experiments. The only requirement is an interface to be used as a generative model for the agent to sample future states and observations during planning. The exact composition of the driver model state can be arbitrary, as the agent never evaluates the driver model state directly but only stores it in a belief particle to be used in the next forward search.

\subsection{Continuous action and observation space}
\label{sec:conclusion-continuous}

We utilize discretization to be able to use POMCP with naturally continuous action and observation spaces (see section \ref{sec:discretization}). Discretization is necessary for POMCP to work. However, it comes with the drawback of a loss of precision. Even if one would increase the resolution of the discretization - using more bins, some degree of information loss is inevitable. The alternative would be to work with the continuous spaces directly. A switch from POMCP to another POMDP solver would be required.

Various solvers have been developed to work with fully continuous POMDP. Two algorithms worth mentioning are POMCPOW, an extension of POMCP using progressive widening \parencite{online_pomdp_cont}, and LABECOP, which, in contrast to POMCPOW, avoids limiting the number of observations considered during planning \parencite{online-cont-pomdp-2}. However, both methods require the observation probabilities $Z$ to be explicitly known, which is not the case in our scenario. One would need to find a way to circumvent this problem. A potential approach would be to approximate the observation probability distribution.

\subsection{Use of active probing}

Human drivers likely react to the actions of the agent, with potentially different reaction times or behavior for distracted drivers. The driver models used in our experiments did not account for this. \cite{att_intersec} show that actively probing human drivers can reduce the uncertainty about their mental state. Enabling the agent to gather information about the human by actively probing her might help to improve the agent's performance. In a scenario with a human driver, the internal state of the human is very complex and therefore hard to estimate. Active probing, such as intentional small steering actions, even if they are not necessary, could be used to gauge the state of the driver more accurately.

%%%%%%
% Step further: Active probing 
% Although efficient, these approximations sacrifice animportant aspect of POMDPs: the ability toactively gatherinformation.

% Intersection, probing other cars: \cite{att_intersec}
% Planning for cars that coordinate with people: leveraging effects onhuman actions for planning and active information gathering overhuman internal state
% Information Gathering Actions over Human Internal State
% -> Our key insight is that robots can leverage their own actions to help estimation of human internalstate.\

%%%%%%
% Step further: Bi-directional feedbackk

% \cite{hitl_pomdp} uses feedback
% Sharing Control With Haptics: Seamless Driver Support From Manual to Automatic Control